[2025-04-22 22:52:35,833] [[32m    INFO[0m]: PyTorch version 2.5.1 available. (config.py:58)[0m
INFO 04-22 22:52:41 __init__.py:190] Automatically detected platform cuda.
[2025-04-22 22:52:41,921] [[32m    INFO[0m]: --- LOADING MODEL --- (pipeline.py:189)[0m
[2025-04-22 22:52:50,055] [[32m    INFO[0m]: This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'. (config.py:542)[0m
[2025-04-22 22:52:50,057] [[32m    INFO[0m]: Initializing a V0 LLM engine (v0.7.2) with config: model='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":128}, use_cached_outputs=False,  (llm_engine.py:234)[0m
[2025-04-22 22:52:51,178] [[32m    INFO[0m]: Using Flash Attention backend. (cuda.py:230)[0m
[2025-04-22 22:52:51,924] [[32m    INFO[0m]: Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B... (model_runner.py:1110)[0m

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.69s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.08s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.02s/it]

[2025-04-22 22:52:58,635] [[32m    INFO[0m]: Loading model weights took 14.2717 GB (model_runner.py:1115)[0m
[2025-04-22 22:52:59,684] [[33m WARNING[0m]: ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py:1832: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  lm_head_weight = torch.load(lm_local, map_location=hidden_states_cur.device).to(dtype=torch.bfloat16)
 (warnings.py:110)[0m
[2025-04-22 22:53:00,537] [[32m    INFO[0m]: Memory profiling takes 1.67 seconds
the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
model weights take 14.27GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 48.11GiB; the rest of the memory reserved for KV Cache is 8.76GiB. (worker.py:267)[0m
[2025-04-22 22:53:00,743] [[32m    INFO[0m]: # CUDA blocks: 10246, # CPU blocks: 4681 (executor_base.py:110)[0m
[2025-04-22 22:53:00,744] [[32m    INFO[0m]: Maximum concurrency for 32768 tokens per request: 5.00x (executor_base.py:115)[0m
[2025-04-22 22:53:03,501] [[32m    INFO[0m]: Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage. (model_runner.py:1434)[0m
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([32768, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])

Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   5%|▌         | 1/19 [00:00<00:09,  1.90it/s]
Capturing CUDA graph shapes:  11%|█         | 2/19 [00:00<00:07,  2.16it/s]
Capturing CUDA graph shapes:  16%|█▌        | 3/19 [00:01<00:07,  2.28it/s]
Capturing CUDA graph shapes:  21%|██        | 4/19 [00:01<00:06,  2.33it/s]
Capturing CUDA graph shapes:  26%|██▋       | 5/19 [00:02<00:06,  2.31it/s]
Capturing CUDA graph shapes:  32%|███▏      | 6/19 [00:02<00:05,  2.30it/s]
Capturing CUDA graph shapes:  37%|███▋      | 7/19 [00:03<00:05,  2.28it/s]
Capturing CUDA graph shapes:  42%|████▏     | 8/19 [00:03<00:04,  2.27it/s]
Capturing CUDA graph shapes:  47%|████▋     | 9/19 [00:03<00:04,  2.29it/s]
Capturing CUDA graph shapes:  53%|█████▎    | 10/19 [00:04<00:03,  2.25it/s]
Capturing CUDA graph shapes:  58%|█████▊    | 11/19 [00:04<00:03,  2.18it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 12/19 [00:05<00:03,  2.22it/s]
Capturing CUDA graph shapes:  68%|██████▊   | 13/19 [00:05<00:02,  2.23it/s]
Capturing CUDA graph shapes:  74%|███████▎  | 14/19 [00:06<00:02,  2.13it/s]
Capturing CUDA graph shapes:  79%|███████▉  | 15/19 [00:06<00:01,  2.04it/s]
Capturing CUDA graph shapes:  84%|████████▍ | 16/19 [00:07<00:01,  1.89it/s]
Capturing CUDA graph shapes:  89%|████████▉ | 17/19 [00:07<00:01,  1.96it/s]
Capturing CUDA graph shapes:  95%|█████████▍| 18/19 [00:08<00:00,  2.08it/s]
Capturing CUDA graph shapes: 100%|██████████| 19/19 [00:08<00:00,  2.11it/s]
Capturing CUDA graph shapes: 100%|██████████| 19/19 [00:08<00:00,  2.16it/s]
[2025-04-22 22:53:12,311] [[32m    INFO[0m]: Graph capturing finished in 9 secs, took 0.22 GiB (model_runner.py:1562)[0m
[2025-04-22 22:53:12,311] [[32m    INFO[0m]: init engine (profile, create kv cache, warmup model) took 13.68 seconds (llm_engine.py:431)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: --- INIT SEEDS --- (pipeline.py:263)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: --- LOADING TASKS --- (pipeline.py:216)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/ifeval/main.py (registry.py:142)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: Found 6 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/tiny_benchmarks/main.py (registry.py:142)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mt_bench/main.py (registry.py:142)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: Found 4 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mix_eval/main.py (registry.py:142)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: Found 5 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/olympiade_bench/main.py (registry.py:142)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/hle/main.py (registry.py:142)[0m
[2025-04-22 22:53:12,353] [[32m    INFO[0m]: Found 21 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/lcb/main.py (registry.py:142)[0m
[2025-04-22 22:53:12,355] [[32m    INFO[0m]: HuggingFaceH4/aime_2024 default (lighteval_task.py:187)[0m
[2025-04-22 22:53:12,355] [[33m WARNING[0m]: Careful, the task lighteval|aime24 is using evaluation data to build the few shot examples. (lighteval_task.py:260)[0m
Using the latest cached version of the dataset since HuggingFaceH4/aime_2024 couldn't be found on the Hugging Face Hub
[2025-04-22 22:53:37,082] [[33m WARNING[0m]: Using the latest cached version of the dataset since HuggingFaceH4/aime_2024 couldn't be found on the Hugging Face Hub (load.py:1631)[0m
Found the latest cached dataset configuration 'default' at ~/.cache/huggingface/datasets/HuggingFaceH4___aime_2024/default/0.0.0/2fe88a2f1091d5048c0f36abc874fb997b3dd99a (last modified on Tue Apr  8 15:12:43 2025).
[2025-04-22 22:53:37,083] [[33m WARNING[0m]: Found the latest cached dataset configuration 'default' at ~/.cache/huggingface/datasets/HuggingFaceH4___aime_2024/default/0.0.0/2fe88a2f1091d5048c0f36abc874fb997b3dd99a (last modified on Tue Apr  8 15:12:43 2025). (cache.py:95)[0m
[2025-04-22 22:53:37,110] [[32m    INFO[0m]: --- RUNNING MODEL --- (pipeline.py:468)[0m
[2025-04-22 22:53:37,110] [[32m    INFO[0m]: Running RequestType.GREEDY_UNTIL requests (pipeline.py:472)[0m
[2025-04-22 22:53:37,116] [[33m WARNING[0m]: You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring. (data.py:260)[0m
HuggingFaceH4/aime_2024
default
False
None

Splits:   0%|          | 0/1 [00:00<?, ?it/s][2025-04-22 22:53:37,122] [[33m WARNING[0m]: context_size + max_new_tokens=33238 which is greater than self.max_length=32768. Truncating context to 0 tokens. (vllm_model.py:270)[0m


Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A

Processed prompts:   3%|▎         | 1/30 [00:21<10:28, 21.66s/it, est. speed input: 9.70 toks/s, output: 97.78 toks/s][A

Processed prompts:   7%|▋         | 2/30 [00:22<04:29,  9.62s/it, est. speed input: 15.23 toks/s, output: 190.30 toks/s][A

Processed prompts:  10%|█         | 3/30 [00:25<02:56,  6.53s/it, est. speed input: 18.64 toks/s, output: 266.61 toks/s][A

Processed prompts:  13%|█▎        | 4/30 [00:29<02:23,  5.51s/it, est. speed input: 22.94 toks/s, output: 328.29 toks/s][A

Processed prompts:  17%|█▋        | 5/30 [00:33<02:05,  5.03s/it, est. speed input: 26.61 toks/s, output: 384.52 toks/s][A

Processed prompts:  20%|██        | 6/30 [00:41<02:19,  5.83s/it, est. speed input: 24.85 toks/s, output: 411.80 toks/s][A

Processed prompts:  23%|██▎       | 7/30 [00:42<01:41,  4.43s/it, est. speed input: 27.16 toks/s, output: 492.94 toks/s][A

Processed prompts:  27%|██▋       | 8/30 [00:48<01:48,  4.93s/it, est. speed input: 27.41 toks/s, output: 527.73 toks/s][A

Processed prompts:  30%|███       | 9/30 [00:54<01:48,  5.15s/it, est. speed input: 27.17 toks/s, output: 567.25 toks/s][A

Processed prompts:  33%|███▎      | 10/30 [01:03<02:06,  6.33s/it, est. speed input: 26.32 toks/s, output: 580.72 toks/s][A

Processed prompts:  37%|███▋      | 11/30 [01:08<01:50,  5.82s/it, est. speed input: 27.68 toks/s, output: 634.04 toks/s][A

Processed prompts:  40%|████      | 12/30 [01:09<01:18,  4.38s/it, est. speed input: 29.75 toks/s, output: 717.19 toks/s][A

Processed prompts:  43%|████▎     | 13/30 [01:12<01:09,  4.11s/it, est. speed input: 30.10 toks/s, output: 775.70 toks/s][A[2025-04-22 22:55:22,729] [[33m WARNING[0m]: Sequence group 28 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1 (scheduler.py:1560)[0m


Processed prompts:  47%|████▋     | 14/30 [02:05<05:02, 18.93s/it, est. speed input: 18.65 toks/s, output: 534.90 toks/s][A

Processed prompts:  50%|█████     | 15/30 [02:17<04:09, 16.65s/it, est. speed input: 18.26 toks/s, output: 576.27 toks/s][A

Processed prompts:  53%|█████▎    | 16/30 [02:30<03:38, 15.57s/it, est. speed input: 17.75 toks/s, output: 610.39 toks/s][A

Processed prompts:  57%|█████▋    | 17/30 [02:41<03:04, 14.21s/it, est. speed input: 18.24 toks/s, output: 651.54 toks/s][A

Processed prompts:  60%|██████    | 18/30 [02:46<02:18, 11.56s/it, est. speed input: 18.48 toks/s, output: 692.76 toks/s][A

Processed prompts:  63%|██████▎   | 19/30 [02:49<01:38,  8.95s/it, est. speed input: 19.22 toks/s, output: 762.97 toks/s][A

Processed prompts:  67%|██████▋   | 20/30 [02:59<01:32,  9.25s/it, est. speed input: 18.86 toks/s, output: 777.75 toks/s][A

Processed prompts:  70%|███████   | 21/30 [03:09<01:24,  9.41s/it, est. speed input: 18.66 toks/s, output: 814.20 toks/s][A

Processed prompts:  73%|███████▎  | 22/30 [03:15<01:07,  8.42s/it, est. speed input: 20.48 toks/s, output: 868.65 toks/s][A

Processed prompts:  77%|███████▋  | 23/30 [03:22<00:56,  8.03s/it, est. speed input: 20.66 toks/s, output: 917.66 toks/s][A

Processed prompts:  80%|████████  | 24/30 [03:26<00:41,  6.94s/it, est. speed input: 20.96 toks/s, output: 977.49 toks/s][A

Processed prompts:  83%|████████▎ | 25/30 [03:42<00:47,  9.52s/it, est. speed input: 20.53 toks/s, output: 987.60 toks/s][A

Processed prompts:  87%|████████▋ | 26/30 [04:01<00:49, 12.45s/it, est. speed input: 19.59 toks/s, output: 986.02 toks/s][A

Processed prompts:  90%|█████████ | 27/30 [05:15<01:32, 30.83s/it, est. speed input: 15.50 toks/s, output: 828.98 toks/s][A

Processed prompts:  93%|█████████▎| 28/30 [07:09<01:51, 55.73s/it, est. speed input: 11.99 toks/s, output: 684.89 toks/s][A

Processed prompts:  97%|█████████▋| 29/30 [07:09<00:39, 39.11s/it, est. speed input: 12.50 toks/s, output: 760.13 toks/s][A

Processed prompts: 100%|██████████| 30/30 [07:10<00:00, 27.58s/it, est. speed input: 12.83 toks/s, output: 834.75 toks/s][A
Processed prompts: 100%|██████████| 30/30 [07:10<00:00, 14.34s/it, est. speed input: 12.83 toks/s, output: 834.75 toks/s]

Splits: 100%|██████████| 1/1 [07:10<00:00, 430.37s/it]
Splits: 100%|██████████| 1/1 [07:10<00:00, 430.37s/it]
[2025-04-22 23:00:48,089] [[32m    INFO[0m]: --- COMPUTING METRICS --- (pipeline.py:504)[0m
[2025-04-22 23:00:48,379] [[32m    INFO[0m]: --- DISPLAYING RESULTS --- (pipeline.py:546)[0m
[2025-04-22 23:00:48,389] [[32m    INFO[0m]: --- SAVING AND PUSHING RESULTS --- (pipeline.py:536)[0m
[2025-04-22 23:00:48,389] [[32m    INFO[0m]: Saving experiment tracker (evaluation_tracker.py:180)[0m
[2025-04-22 23:00:48,585] [[32m    INFO[0m]: Saving results to ~/Projects/nips25_slot/open-r2/data/evals~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/results/_Models_deepseek-ai_DeepSeek-R1-Distill-Qwen-7B/results_2025-04-22T23-00-48.389427.json (evaluation_tracker.py:234)[0m
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([5522, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([10865, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([11421, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([12094, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([10178, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([9565, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=1, lr=0.1
DEBUG - hidden_states_orig: torch.Size([9710, 3584]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25_slot/open-r2/lm_head/DeepSeek-R1-Distill-Qwen-7B_lm_head.pt
cuda:0 torch.Size([152064, 3584])
|       Task       |Version|     Metric     |Value |   |Stderr|
|------------------|------:|----------------|-----:|---|-----:|
|all               |       |extractive_match|0.5667|±  | 0.092|
|lighteval:aime24:0|      1|extractive_match|0.5667|±  | 0.092|

