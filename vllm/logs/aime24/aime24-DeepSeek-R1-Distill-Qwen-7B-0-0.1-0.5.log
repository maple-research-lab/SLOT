[2025-04-22 22:46:05,877] [[32m    INFO[0m]: PyTorch version 2.5.1 available. (config.py:58)[0m
INFO 04-22 22:46:10 __init__.py:190] Automatically detected platform cuda.
[2025-04-22 22:46:11,368] [[32m    INFO[0m]: --- LOADING MODEL --- (pipeline.py:189)[0m
[2025-04-22 22:46:19,340] [[32m    INFO[0m]: This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'. (config.py:542)[0m
[2025-04-22 22:46:19,343] [[32m    INFO[0m]: Initializing a V0 LLM engine (v0.7.2) with config: model='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":128}, use_cached_outputs=False,  (llm_engine.py:234)[0m
[2025-04-22 22:46:20,321] [[32m    INFO[0m]: Using Flash Attention backend. (cuda.py:230)[0m
[2025-04-22 22:46:20,951] [[32m    INFO[0m]: Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B... (model_runner.py:1110)[0m

Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.26s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.69s/it]

Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.62s/it]

[2025-04-22 22:46:26,659] [[32m    INFO[0m]: Loading model weights took 14.2717 GB (model_runner.py:1115)[0m
[2025-04-22 22:46:27,835] [[32m    INFO[0m]: Memory profiling takes 1.03 seconds
the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
model weights take 14.27GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 4.38GiB; the rest of the memory reserved for KV Cache is 52.49GiB. (worker.py:267)[0m
[2025-04-22 22:46:27,986] [[32m    INFO[0m]: # CUDA blocks: 61428, # CPU blocks: 4681 (executor_base.py:110)[0m
[2025-04-22 22:46:27,986] [[32m    INFO[0m]: Maximum concurrency for 32768 tokens per request: 29.99x (executor_base.py:115)[0m
[2025-04-22 22:46:30,188] [[32m    INFO[0m]: Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage. (model_runner.py:1434)[0m
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1

Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graph shapes:   5%|▌         | 1/19 [00:00<00:07,  2.28it/s]
Capturing CUDA graph shapes:  11%|█         | 2/19 [00:00<00:06,  2.63it/s]
Capturing CUDA graph shapes:  16%|█▌        | 3/19 [00:01<00:05,  2.73it/s]
Capturing CUDA graph shapes:  21%|██        | 4/19 [00:01<00:05,  2.84it/s]
Capturing CUDA graph shapes:  26%|██▋       | 5/19 [00:01<00:04,  2.91it/s]
Capturing CUDA graph shapes:  32%|███▏      | 6/19 [00:02<00:04,  2.94it/s]
Capturing CUDA graph shapes:  37%|███▋      | 7/19 [00:02<00:04,  2.98it/s]
Capturing CUDA graph shapes:  42%|████▏     | 8/19 [00:02<00:03,  2.94it/s]
Capturing CUDA graph shapes:  47%|████▋     | 9/19 [00:03<00:03,  2.89it/s]
Capturing CUDA graph shapes:  53%|█████▎    | 10/19 [00:03<00:03,  2.83it/s]
Capturing CUDA graph shapes:  58%|█████▊    | 11/19 [00:03<00:02,  2.75it/s]
Capturing CUDA graph shapes:  63%|██████▎   | 12/19 [00:04<00:02,  2.77it/s]
Capturing CUDA graph shapes:  68%|██████▊   | 13/19 [00:04<00:02,  2.77it/s]
Capturing CUDA graph shapes:  74%|███████▎  | 14/19 [00:04<00:01,  2.87it/s]
Capturing CUDA graph shapes:  79%|███████▉  | 15/19 [00:05<00:01,  2.83it/s]
Capturing CUDA graph shapes:  84%|████████▍ | 16/19 [00:05<00:01,  2.86it/s]
Capturing CUDA graph shapes:  89%|████████▉ | 17/19 [00:06<00:00,  2.85it/s]
Capturing CUDA graph shapes:  95%|█████████▍| 18/19 [00:06<00:00,  2.79it/s]
Capturing CUDA graph shapes: 100%|██████████| 19/19 [00:06<00:00,  2.81it/s]
Capturing CUDA graph shapes: 100%|██████████| 19/19 [00:06<00:00,  2.82it/s]
[2025-04-22 22:46:36,926] [[32m    INFO[0m]: Graph capturing finished in 7 secs, took 0.22 GiB (model_runner.py:1562)[0m
[2025-04-22 22:46:36,927] [[32m    INFO[0m]: init engine (profile, create kv cache, warmup model) took 10.27 seconds (llm_engine.py:431)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: --- INIT SEEDS --- (pipeline.py:263)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: --- LOADING TASKS --- (pipeline.py:216)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/ifeval/main.py (registry.py:142)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: Found 6 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/tiny_benchmarks/main.py (registry.py:142)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mt_bench/main.py (registry.py:142)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: Found 4 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mix_eval/main.py (registry.py:142)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: Found 5 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/olympiade_bench/main.py (registry.py:142)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/hle/main.py (registry.py:142)[0m
[2025-04-22 22:46:37,526] [[32m    INFO[0m]: Found 21 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/lcb/main.py (registry.py:142)[0m
[2025-04-22 22:46:37,528] [[32m    INFO[0m]: HuggingFaceH4/aime_2024 default (lighteval_task.py:187)[0m
[2025-04-22 22:46:37,528] [[33m WARNING[0m]: Careful, the task lighteval|aime24 is using evaluation data to build the few shot examples. (lighteval_task.py:260)[0m
Using the latest cached version of the dataset since HuggingFaceH4/aime_2024 couldn't be found on the Hugging Face Hub
[2025-04-22 22:46:46,779] [[33m WARNING[0m]: Using the latest cached version of the dataset since HuggingFaceH4/aime_2024 couldn't be found on the Hugging Face Hub (load.py:1631)[0m
Found the latest cached dataset configuration 'default' at ~/.cache/huggingface/datasets/HuggingFaceH4___aime_2024/default/0.0.0/2fe88a2f1091d5048c0f36abc874fb997b3dd99a (last modified on Tue Apr  8 15:12:43 2025).
[2025-04-22 22:46:46,781] [[33m WARNING[0m]: Found the latest cached dataset configuration 'default' at ~/.cache/huggingface/datasets/HuggingFaceH4___aime_2024/default/0.0.0/2fe88a2f1091d5048c0f36abc874fb997b3dd99a (last modified on Tue Apr  8 15:12:43 2025). (cache.py:95)[0m
[2025-04-22 22:46:46,812] [[32m    INFO[0m]: --- RUNNING MODEL --- (pipeline.py:468)[0m
[2025-04-22 22:46:46,812] [[32m    INFO[0m]: Running RequestType.GREEDY_UNTIL requests (pipeline.py:472)[0m
[2025-04-22 22:46:46,818] [[33m WARNING[0m]: You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring. (data.py:260)[0m
HuggingFaceH4/aime_2024
default
False
None

Splits:   0%|          | 0/1 [00:00<?, ?it/s][2025-04-22 22:46:46,824] [[33m WARNING[0m]: context_size + max_new_tokens=33238 which is greater than self.max_length=32768. Truncating context to 0 tokens. (vllm_model.py:270)[0m


Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A

Processed prompts:   3%|▎         | 1/30 [00:17<08:40, 17.94s/it, est. speed input: 11.71 toks/s, output: 102.23 toks/s][A

Processed prompts:   7%|▋         | 2/30 [00:20<04:14,  9.11s/it, est. speed input: 16.11 toks/s, output: 189.50 toks/s][A

Processed prompts:  10%|█         | 3/30 [00:22<02:35,  5.76s/it, est. speed input: 20.95 toks/s, output: 275.82 toks/s][A

Processed prompts:  13%|█▎        | 4/30 [00:23<01:36,  3.70s/it, est. speed input: 29.93 toks/s, output: 370.30 toks/s][A

Processed prompts:  17%|█▋        | 5/30 [00:26<01:28,  3.52s/it, est. speed input: 33.92 toks/s, output: 425.99 toks/s][A

Processed prompts:  20%|██        | 6/30 [00:33<01:51,  4.64s/it, est. speed input: 30.90 toks/s, output: 437.18 toks/s][A

Processed prompts:  23%|██▎       | 7/30 [00:53<03:42,  9.67s/it, est. speed input: 21.61 toks/s, output: 368.92 toks/s][A

Processed prompts:  27%|██▋       | 8/30 [00:59<03:10,  8.66s/it, est. speed input: 21.55 toks/s, output: 424.14 toks/s][A

Processed prompts:  30%|███       | 9/30 [01:03<02:28,  7.08s/it, est. speed input: 22.57 toks/s, output: 494.81 toks/s][A

Processed prompts:  33%|███▎      | 10/30 [01:09<02:14,  6.75s/it, est. speed input: 23.13 toks/s, output: 546.04 toks/s][A

Processed prompts:  37%|███▋      | 11/30 [01:10<01:37,  5.13s/it, est. speed input: 24.48 toks/s, output: 628.69 toks/s][A

Processed prompts:  40%|████      | 12/30 [01:12<01:13,  4.09s/it, est. speed input: 26.36 toks/s, output: 707.73 toks/s][A

Processed prompts:  43%|████▎     | 13/30 [01:48<03:54, 13.77s/it, est. speed input: 19.11 toks/s, output: 562.28 toks/s][A

Processed prompts:  47%|████▋     | 14/30 [01:51<02:49, 10.61s/it, est. speed input: 19.91 toks/s, output: 634.65 toks/s][A

Processed prompts:  50%|█████     | 15/30 [01:52<01:54,  7.64s/it, est. speed input: 21.70 toks/s, output: 719.49 toks/s][A

Processed prompts:  53%|█████▎    | 16/30 [01:57<01:35,  6.82s/it, est. speed input: 22.09 toks/s, output: 777.75 toks/s][A

Processed prompts:  57%|█████▋    | 17/30 [02:00<01:14,  5.70s/it, est. speed input: 22.83 toks/s, output: 845.88 toks/s][A

Processed prompts:  60%|██████    | 18/30 [02:02<00:53,  4.50s/it, est. speed input: 23.77 toks/s, output: 922.24 toks/s][A

Processed prompts:  63%|██████▎   | 19/30 [02:05<00:45,  4.14s/it, est. speed input: 24.31 toks/s, output: 985.52 toks/s][A

Processed prompts:  67%|██████▋   | 20/30 [02:15<00:58,  5.80s/it, est. speed input: 23.77 toks/s, output: 1001.81 toks/s][A

Processed prompts:  70%|███████   | 21/30 [02:16<00:40,  4.49s/it, est. speed input: 24.76 toks/s, output: 1077.92 toks/s][A

Processed prompts:  73%|███████▎  | 22/30 [02:33<01:06,  8.25s/it, est. speed input: 23.50 toks/s, output: 1043.37 toks/s][A

Processed prompts:  77%|███████▋  | 23/30 [02:34<00:41,  5.96s/it, est. speed input: 24.59 toks/s, output: 1124.07 toks/s][A

Processed prompts:  80%|████████  | 24/30 [02:46<00:46,  7.75s/it, est. speed input: 23.67 toks/s, output: 1127.32 toks/s][A

Processed prompts:  83%|████████▎ | 25/30 [02:50<00:32,  6.59s/it, est. speed input: 24.63 toks/s, output: 1185.20 toks/s][A

Processed prompts:  87%|████████▋ | 26/30 [02:55<00:24,  6.08s/it, est. speed input: 24.93 toks/s, output: 1235.25 toks/s][A

Processed prompts:  90%|█████████ | 27/30 [03:01<00:18,  6.27s/it, est. speed input: 26.60 toks/s, output: 1272.53 toks/s][A

Processed prompts:  93%|█████████▎| 28/30 [03:04<00:10,  5.13s/it, est. speed input: 27.27 toks/s, output: 1338.38 toks/s][A

Processed prompts:  97%|█████████▋| 29/30 [03:59<00:20, 20.07s/it, est. speed input: 21.94 toks/s, output: 1119.45 toks/s][A

Processed prompts: 100%|██████████| 30/30 [05:44<00:00, 45.50s/it, est. speed input: 16.05 toks/s, output: 872.78 toks/s] [A
Processed prompts: 100%|██████████| 30/30 [05:44<00:00, 11.47s/it, est. speed input: 16.05 toks/s, output: 872.78 toks/s]

Splits: 100%|██████████| 1/1 [05:44<00:00, 344.14s/it]
Splits: 100%|██████████| 1/1 [05:44<00:00, 344.14s/it]
[2025-04-22 22:52:31,621] [[32m    INFO[0m]: --- COMPUTING METRICS --- (pipeline.py:504)[0m
[2025-04-22 22:52:31,878] [[32m    INFO[0m]: --- DISPLAYING RESULTS --- (pipeline.py:546)[0m
[2025-04-22 22:52:31,887] [[32m    INFO[0m]: --- SAVING AND PUSHING RESULTS --- (pipeline.py:536)[0m
[2025-04-22 22:52:31,887] [[32m    INFO[0m]: Saving experiment tracker (evaluation_tracker.py:180)[0m
[2025-04-22 22:52:32,054] [[32m    INFO[0m]: Saving results to ~/Projects/nips25_slot/open-r2/data/evals~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/results/_Models_deepseek-ai_DeepSeek-R1-Distill-Qwen-7B/results_2025-04-22T22-52-31.888003.json (evaluation_tracker.py:234)[0m
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
|       Task       |Version|     Metric     |Value|   |Stderr|
|------------------|------:|----------------|----:|---|-----:|
|all               |       |extractive_match|  0.5|±  |0.0928|
|lighteval:aime24:0|      1|extractive_match|  0.5|±  |0.0928|

