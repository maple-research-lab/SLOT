[2025-04-11 20:28:34,367] [[32m    INFO[0m]: PyTorch version 2.5.1 available. (config.py:54)[0m
INFO 04-11 20:28:41 __init__.py:190] Automatically detected platform cuda.
[2025-04-11 20:28:42,347] [[32m    INFO[0m]: --- LOADING MODEL --- (pipeline.py:189)[0m
[2025-04-11 20:28:50,299] [[32m    INFO[0m]: This model supports multiple tasks: {'reward', 'embed', 'classify', 'score', 'generate'}. Defaulting to 'generate'. (config.py:542)[0m
[2025-04-11 20:28:50,483] [[32m    INFO[0m]: Defaulting to use mp for distributed inference (config.py:1401)[0m
[2025-04-11 20:28:50,487] [[32m    INFO[0m]: Initializing a V0 LLM engine (v0.7.2) with config: model='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":128}, use_cached_outputs=False,  (llm_engine.py:234)[0m
[2025-04-11 20:28:50,791] [[33m WARNING[0m]: Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed. (multiproc_worker_utils.py:300)[0m
[2025-04-11 20:28:50,806] [[32m    INFO[0m]: Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager (custom_cache_manager.py:19)[0m
[2025-04-11 20:28:51,508] [[32m    INFO[0m]: Using Flash Attention backend. (cuda.py:230)[0m
INFO 04-11 20:28:55 __init__.py:190] Automatically detected platform cuda.
INFO 04-11 20:28:55 __init__.py:190] Automatically detected platform cuda.
INFO 04-11 20:28:55 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:28:57 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:28:57 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:28:57 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:00 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:00 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:00 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:01 utils.py:950] Found nccl from library libnccl.so.2
[2025-04-11 20:29:01,521] [[32m    INFO[0m]: Found nccl from library libnccl.so.2 (utils.py:950)[0m
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:01 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:01 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:01 pynccl.py:69] vLLM is using nccl==2.21.5
[2025-04-11 20:29:01,522] [[32m    INFO[0m]: vLLM is using nccl==2.21.5 (pynccl.py:69)[0m
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:01 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:01 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:05 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:05 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:05 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
[2025-04-11 20:29:05,289] [[32m    INFO[0m]: reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json (custom_all_reduce_utils.py:244)[0m
[2025-04-11 20:29:05,396] [[32m    INFO[0m]: vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d101435e'), local_subscribe_port=44741, remote_subscribe_port=None) (shm_broadcast.py:258)[0m
[2025-04-11 20:29:05,420] [[32m    INFO[0m]: Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B... (model_runner.py:1110)[0m
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:05 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:05 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:05 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...

Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]

Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]

[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:06 model_runner.py:1115] Loading model weights took 0.8985 GB
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:06 model_runner.py:1115] Loading model weights took 0.8985 GB
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:06 model_runner.py:1115] Loading model weights took 0.8985 GB
[2025-04-11 20:29:06,382] [[32m    INFO[0m]: Loading model weights took 0.8985 GB (model_runner.py:1115)[0m
[2025-04-11 20:29:08,657] [[33m WARNING[0m]: ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py:1828: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  lm_head_weight = torch.load(lm_local, map_location=hidden_states_cur.device).to(dtype=torch.bfloat16)
 (warnings.py:110)[0m
[1;36m(VllmWorkerProcess pid=4145798)[0;0m [2025-04-11 20:29:08,657] [[33m WARNING[0m]: ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py:1828: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[1;36m(VllmWorkerProcess pid=4145799)[0;0m [2025-04-11 20:29:08,657] [[33m WARNING[0m]: ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py:1828: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[1;36m(VllmWorkerProcess pid=4145800)[0;0m [2025-04-11 20:29:08,657] [[33m WARNING[0m]: ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py:1828: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[1;36m(VllmWorkerProcess pid=4145798)[0;0m   lm_head_weight = torch.load(lm_local, map_location=hidden_states_cur.device).to(dtype=torch.bfloat16)
[1;36m(VllmWorkerProcess pid=4145799)[0;0m   lm_head_weight = torch.load(lm_local, map_location=hidden_states_cur.device).to(dtype=torch.bfloat16)
[1;36m(VllmWorkerProcess pid=4145800)[0;0m   lm_head_weight = torch.load(lm_local, map_location=hidden_states_cur.device).to(dtype=torch.bfloat16)
[1;36m(VllmWorkerProcess pid=4145798)[0;0m  (warnings.py:110)[0m
[1;36m(VllmWorkerProcess pid=4145799)[0;0m  (warnings.py:110)[0m
[1;36m(VllmWorkerProcess pid=4145800)[0;0m  (warnings.py:110)[0m
[1;36m(VllmWorkerProcess pid=4145799)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=4145799)[0;0m DEBUG - CHOT parameters: steps=5, lr=1.0
[1;36m(VllmWorkerProcess pid=4145799)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145799)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145799)[0;0m cuda:2 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145799)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145799)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145799)[0;0m cuda:2 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145799)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145799)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145799)[0;0m cuda:2 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145799)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145799)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145799)[0;0m cuda:2 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145799)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145799)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145799)[0;0m cuda:2 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:11 worker.py:267] Memory profiling takes 4.90 seconds
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:11 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.80) = 63.37GiB
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:11 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 2.38GiB; PyTorch activation peak memory takes 47.12GiB; the rest of the memory reserved for KV Cache is 12.97GiB.
[1;36m(VllmWorkerProcess pid=4145798)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=4145798)[0;0m DEBUG - CHOT parameters: steps=5, lr=1.0
[1;36m(VllmWorkerProcess pid=4145798)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145798)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145798)[0;0m cuda:1 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145798)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145798)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145798)[0;0m cuda:1 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145798)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145798)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145798)[0;0m cuda:1 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145798)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145798)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145798)[0;0m cuda:1 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145798)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145798)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145798)[0;0m cuda:1 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:11 worker.py:267] Memory profiling takes 4.88 seconds
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:11 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.80) = 63.37GiB
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:11 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 2.38GiB; PyTorch activation peak memory takes 47.12GiB; the rest of the memory reserved for KV Cache is 12.97GiB.
[1;36m(VllmWorkerProcess pid=4145800)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=4145800)[0;0m DEBUG - CHOT parameters: steps=5, lr=1.0
[1;36m(VllmWorkerProcess pid=4145800)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145800)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145800)[0;0m cuda:3 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145800)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145800)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145800)[0;0m cuda:3 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145800)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145800)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145800)[0;0m cuda:3 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145800)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145800)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145800)[0;0m cuda:3 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145800)[0;0m DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
[1;36m(VllmWorkerProcess pid=4145800)[0;0m Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
[1;36m(VllmWorkerProcess pid=4145800)[0;0m cuda:3 torch.Size([151936, 1536])
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:11 worker.py:267] Memory profiling takes 4.89 seconds
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:11 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.80) = 63.37GiB
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:11 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 2.38GiB; PyTorch activation peak memory takes 47.12GiB; the rest of the memory reserved for KV Cache is 12.97GiB.
[2025-04-11 20:29:11,508] [[32m    INFO[0m]: Memory profiling takes 4.92 seconds
the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.80) = 63.37GiB
model weights take 0.90GiB; non_torch_memory takes 2.82GiB; PyTorch activation peak memory takes 47.12GiB; the rest of the memory reserved for KV Cache is 12.53GiB. (worker.py:267)[0m
[2025-04-11 20:29:11,722] [[32m    INFO[0m]: # CUDA blocks: 58670, # CPU blocks: 18724 (executor_base.py:110)[0m
[2025-04-11 20:29:11,722] [[32m    INFO[0m]: Maximum concurrency for 32768 tokens per request: 28.65x (executor_base.py:115)[0m
[2025-04-11 20:29:14,382] [[32m    INFO[0m]: Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage. (model_runner.py:1434)[0m
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=5, lr=1.0
DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([32768, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])

Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s][1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:14 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   5%|â–Œ         | 1/19 [00:00<00:08,  2.13it/s]
Capturing CUDA graph shapes:  11%|â–ˆ         | 2/19 [00:00<00:07,  2.13it/s]
Capturing CUDA graph shapes:  16%|â–ˆâ–Œ        | 3/19 [00:01<00:07,  2.15it/s]
Capturing CUDA graph shapes:  21%|â–ˆâ–ˆ        | 4/19 [00:01<00:06,  2.17it/s]
Capturing CUDA graph shapes:  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:02<00:06,  2.16it/s]
Capturing CUDA graph shapes:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:02<00:05,  2.18it/s]
Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:03<00:05,  2.18it/s]
Capturing CUDA graph shapes:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:03<00:05,  2.18it/s]
Capturing CUDA graph shapes:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:04<00:04,  2.18it/s]
Capturing CUDA graph shapes:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:04<00:04,  2.13it/s]
Capturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [00:05<00:03,  2.13it/s]
Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:05<00:03,  2.14it/s]
Capturing CUDA graph shapes:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:06<00:02,  2.15it/s][1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:20 custom_all_reduce.py:226] Registering 1083 cuda graph addresses

Capturing CUDA graph shapes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:06<00:02,  2.12it/s]
Capturing CUDA graph shapes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:06<00:01,  2.13it/s]
Capturing CUDA graph shapes:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:07<00:01,  2.17it/s][1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:22 custom_all_reduce.py:226] Registering 1083 cuda graph addresses

Capturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:07<00:00,  2.25it/s][1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:22 custom_all_reduce.py:226] Registering 1083 cuda graph addresses

Capturing CUDA graph shapes:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:08<00:00,  2.35it/s]
Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:08<00:00,  2.09it/s]
Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:08<00:00,  2.16it/s]
[2025-04-11 20:29:23,200] [[32m    INFO[0m]: Registering 1083 cuda graph addresses (custom_all_reduce.py:226)[0m
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:29:23 model_runner.py:1562] Graph capturing finished in 9 secs, took 0.15 GiB
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:29:23 model_runner.py:1562] Graph capturing finished in 9 secs, took 0.15 GiB
[2025-04-11 20:29:23,249] [[32m    INFO[0m]: Graph capturing finished in 9 secs, took 0.15 GiB (model_runner.py:1562)[0m
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:29:23 model_runner.py:1562] Graph capturing finished in 9 secs, took 0.15 GiB
[2025-04-11 20:29:23,253] [[32m    INFO[0m]: init engine (profile, create kv cache, warmup model) took 16.87 seconds (llm_engine.py:431)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: --- INIT SEEDS --- (pipeline.py:263)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: --- LOADING TASKS --- (pipeline.py:216)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/ifeval/main.py (registry.py:142)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: Found 6 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/tiny_benchmarks/main.py (registry.py:142)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mt_bench/main.py (registry.py:142)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: Found 4 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mix_eval/main.py (registry.py:142)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: Found 5 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/olympiade_bench/main.py (registry.py:142)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/hle/main.py (registry.py:142)[0m
[2025-04-11 20:29:23,886] [[32m    INFO[0m]: Found 21 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/lcb/main.py (registry.py:142)[0m
[2025-04-11 20:29:23,888] [[32m    INFO[0m]: HuggingFaceH4/aime_2024 default (lighteval_task.py:187)[0m
[2025-04-11 20:29:23,888] [[33m WARNING[0m]: Careful, the task lighteval|aime24 is using evaluation data to build the few shot examples. (lighteval_task.py:260)[0m
Using the latest cached version of the dataset since HuggingFaceH4/aime_2024 couldn't be found on the Hugging Face Hub
[2025-04-11 20:30:49,857] [[33m WARNING[0m]: Using the latest cached version of the dataset since HuggingFaceH4/aime_2024 couldn't be found on the Hugging Face Hub (load.py:1377)[0m
Found the latest cached dataset configuration 'default' at ~/.cache/huggingface/datasets/HuggingFaceH4___aime_2024/default/0.0.0/2fe88a2f1091d5048c0f36abc874fb997b3dd99a (last modified on Tue Apr  8 15:12:43 2025).
[2025-04-11 20:30:49,859] [[33m WARNING[0m]: Found the latest cached dataset configuration 'default' at ~/.cache/huggingface/datasets/HuggingFaceH4___aime_2024/default/0.0.0/2fe88a2f1091d5048c0f36abc874fb997b3dd99a (last modified on Tue Apr  8 15:12:43 2025). (cache.py:94)[0m
[2025-04-11 20:30:49,886] [[32m    INFO[0m]: --- RUNNING MODEL --- (pipeline.py:468)[0m
[2025-04-11 20:30:49,886] [[32m    INFO[0m]: Running RequestType.GREEDY_UNTIL requests (pipeline.py:472)[0m
[2025-04-11 20:30:49,892] [[33m WARNING[0m]: You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring. (data.py:260)[0m

Splits:   0%|          | 0/1 [00:00<?, ?it/s][2025-04-11 20:30:49,898] [[33m WARNING[0m]: context_size + max_new_tokens=33238 which is greater than self.max_length=32768. Truncating context to 0 tokens. (vllm_model.py:270)[0m


Processed prompts:   0%|          | 0/30 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A[2025-04-11 20:30:50,028] [[33m WARNING[0m]: ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/vllm/worker/model_runner.py:1828: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  lm_head_weight = torch.load(lm_local, map_location=hidden_states_cur.device).to(dtype=torch.bfloat16)
 (warnings.py:110)[0m


Processed prompts:   3%|â–Ž         | 1/30 [00:15<07:25, 15.36s/it, est. speed input: 13.09 toks/s, output: 165.41 toks/s][A

Processed prompts:   7%|â–‹         | 2/30 [00:16<03:19,  7.11s/it, est. speed input: 24.62 toks/s, output: 318.44 toks/s][A

Processed prompts:  10%|â–ˆ         | 3/30 [00:29<04:26,  9.85s/it, est. speed input: 18.42 toks/s, output: 347.07 toks/s][A

Processed prompts:  13%|â–ˆâ–Ž        | 4/30 [00:34<03:24,  7.86s/it, est. speed input: 22.21 toks/s, output: 467.29 toks/s][A

Processed prompts:  17%|â–ˆâ–‹        | 5/30 [00:35<02:13,  5.33s/it, est. speed input: 25.55 toks/s, output: 624.70 toks/s][A

Processed prompts:  20%|â–ˆâ–ˆ        | 6/30 [00:37<01:44,  4.35s/it, est. speed input: 27.17 toks/s, output: 752.79 toks/s][A

Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:40<01:26,  3.74s/it, est. speed input: 29.83 toks/s, output: 874.87 toks/s][A

Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:42<01:10,  3.22s/it, est. speed input: 32.55 toks/s, output: 1000.49 toks/s][A

Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:46<01:15,  3.59s/it, est. speed input: 35.34 toks/s, output: 1073.52 toks/s][A

Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [01:08<03:02,  9.15s/it, est. speed input: 26.10 toks/s, output: 878.34 toks/s][A

Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [01:09<02:07,  6.70s/it, est. speed input: 27.87 toks/s, output: 1006.26 toks/s][A

Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [01:21<02:28,  8.23s/it, est. speed input: 25.60 toks/s, output: 996.86 toks/s] [A

Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [01:21<01:39,  5.86s/it, est. speed input: 27.05 toks/s, output: 1127.32 toks/s][A

Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [01:27<01:33,  5.85s/it, est. speed input: 26.99 toks/s, output: 1184.89 toks/s][A

Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [01:31<01:18,  5.24s/it, est. speed input: 27.59 toks/s, output: 1266.61 toks/s][A

Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [01:36<01:12,  5.21s/it, est. speed input: 27.43 toks/s, output: 1328.87 toks/s][A

Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [01:51<01:47,  8.25s/it, est. speed input: 25.19 toks/s, output: 1272.72 toks/s][A

Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [02:10<02:16, 11.35s/it, est. speed input: 23.57 toks/s, output: 1213.98 toks/s][A

Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [02:20<02:01, 11.04s/it, est. speed input: 23.43 toks/s, output: 1245.82 toks/s][A

Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [02:30<01:46, 10.62s/it, est. speed input: 23.37 toks/s, output: 1285.46 toks/s][A

Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [02:38<01:28,  9.89s/it, est. speed input: 23.35 toks/s, output: 1338.03 toks/s][A

Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [02:57<01:41, 12.74s/it, est. speed input: 21.68 toks/s, output: 1309.55 toks/s][A

Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [03:13<01:35, 13.61s/it, est. speed input: 20.68 toks/s, output: 1320.01 toks/s][A

Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [03:16<01:01, 10.26s/it, est. speed input: 21.13 toks/s, output: 1419.88 toks/s][A

Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [03:30<00:58, 11.62s/it, est. speed input: 21.88 toks/s, output: 1436.00 toks/s][A

Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [03:33<00:35,  8.83s/it, est. speed input: 22.41 toks/s, output: 1536.05 toks/s][A

Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [03:39<00:23,  7.99s/it, est. speed input: 22.58 toks/s, output: 1609.63 toks/s][A

Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [03:46<00:15,  7.92s/it, est. speed input: 22.52 toks/s, output: 1670.43 toks/s][A

Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [04:26<00:17, 17.46s/it, est. speed input: 19.85 toks/s, output: 1538.17 toks/s][A

Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [04:37<00:00, 15.52s/it, est. speed input: 19.89 toks/s, output: 1594.52 toks/s][A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [04:37<00:00,  9.26s/it, est. speed input: 19.89 toks/s, output: 1594.52 toks/s]

Splits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:37<00:00, 277.82s/it]
Splits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:37<00:00, 277.82s/it]
[2025-04-11 20:35:28,410] [[32m    INFO[0m]: Terminating local vLLM worker processes (multiproc_worker_utils.py:141)[0m
[1;36m(VllmWorkerProcess pid=4145800)[0;0m INFO 04-11 20:35:28 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=4145798)[0;0m INFO 04-11 20:35:28 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=4145799)[0;0m INFO 04-11 20:35:28 multiproc_worker_utils.py:253] Worker exiting
[2025-04-11 20:35:29,084] [[32m    INFO[0m]: --- COMPUTING METRICS --- (pipeline.py:504)[0m
[2025-04-11 20:35:29,369] [[32m    INFO[0m]: --- DISPLAYING RESULTS --- (pipeline.py:546)[0m
[2025-04-11 20:35:29,379] [[32m    INFO[0m]: --- SAVING AND PUSHING RESULTS --- (pipeline.py:536)[0m
[2025-04-11 20:35:29,380] [[32m    INFO[0m]: Saving experiment tracker (evaluation_tracker.py:180)[0m
[2025-04-11 20:35:29,619] [[32m    INFO[0m]: Saving results to ~/Projects/nips25_slot/open-r2/data/evals~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/results/_Models_deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B/results_2025-04-11T20-35-29.380159.json (evaluation_tracker.py:234)[0m
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=5, lr=1.0
DEBUG - hidden_states_orig: torch.Size([5522, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([5522, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([5522, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([5522, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
DEBUG - hidden_states_orig: torch.Size([5522, 1536]), torch.bfloat16
Loading lm_local from: ~/Projects/nips25/LongRL_causvid/DeepSeek-R1-Distill-Qwen-1.5B_lm_head.pt
cuda:0 torch.Size([151936, 1536])
|       Task       |Version|     Metric     |Value|   |Stderr|
|------------------|------:|----------------|----:|---|-----:|
|all               |       |extractive_match|  0.3|Â±  |0.0851|
|lighteval:aime24:0|      1|extractive_match|  0.3|Â±  |0.0851|

