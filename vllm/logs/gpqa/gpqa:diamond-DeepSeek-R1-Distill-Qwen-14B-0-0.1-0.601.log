[2025-04-15 01:46:34,731] [[32m    INFO[0m]: PyTorch version 2.5.1 available. (config.py:58)[0m
INFO 04-15 01:46:42 __init__.py:190] Automatically detected platform cuda.
[2025-04-15 01:46:43,693] [[32m    INFO[0m]: --- LOADING MODEL --- (pipeline.py:189)[0m
[2025-04-15 01:46:52,587] [[32m    INFO[0m]: This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify', 'score'}. Defaulting to 'generate'. (config.py:542)[0m
[2025-04-15 01:46:52,874] [[32m    INFO[0m]: Defaulting to use mp for distributed inference (config.py:1401)[0m
[2025-04-15 01:46:52,878] [[32m    INFO[0m]: Initializing a V0 LLM engine (v0.7.2) with config: model='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', speculative_config=None, tokenizer='~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=main, override_neuron_config=None, tokenizer_revision=main, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":128}, use_cached_outputs=False,  (llm_engine.py:234)[0m
[2025-04-15 01:46:53,153] [[33m WARNING[0m]: Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed. (multiproc_worker_utils.py:300)[0m
[2025-04-15 01:46:53,168] [[32m    INFO[0m]: Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager (custom_cache_manager.py:19)[0m
[2025-04-15 01:46:53,869] [[32m    INFO[0m]: Using Flash Attention backend. (cuda.py:230)[0m
INFO 04-15 01:47:00 __init__.py:190] Automatically detected platform cuda.
INFO 04-15 01:47:00 __init__.py:190] Automatically detected platform cuda.
INFO 04-15 01:47:00 __init__.py:190] Automatically detected platform cuda.
INFO 04-15 01:47:00 __init__.py:190] Automatically detected platform cuda.
INFO 04-15 01:47:00 __init__.py:190] Automatically detected platform cuda.
INFO 04-15 01:47:00 __init__.py:190] Automatically detected platform cuda.
INFO 04-15 01:47:00 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:09 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:09 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:09 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:09 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:09 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:09 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:09 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:13 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:13 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:13 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:13 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:13 pynccl.py:69] vLLM is using nccl==2.21.5
[2025-04-15 01:47:13,112] [[32m    INFO[0m]: Found nccl from library libnccl.so.2 (utils.py:950)[0m
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:13 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:13 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:13 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:13 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:13 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:13 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:13 pynccl.py:69] vLLM is using nccl==2.21.5
[2025-04-15 01:47:13,113] [[32m    INFO[0m]: vLLM is using nccl==2.21.5 (pynccl.py:69)[0m
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:13 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:13 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:20 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:20 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:20 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[2025-04-15 01:47:20,841] [[32m    INFO[0m]: reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json (custom_all_reduce_utils.py:244)[0m
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:20 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:20 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:20 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:20 custom_all_reduce_utils.py:244] reading GPU P2P access cache from ~/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json
[2025-04-15 01:47:21,279] [[32m    INFO[0m]: vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_4491a417'), local_subscribe_port=35991, remote_subscribe_port=None) (shm_broadcast.py:258)[0m
[2025-04-15 01:47:21,289] [[32m    INFO[0m]: Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B... (model_runner.py:1110)[0m
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:21 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B...
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:21 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B...
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:21 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B...
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:21 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B...
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:21 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B...
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:21 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B...
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:21 model_runner.py:1110] Starting to load model ~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B...

Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:02<00:08,  2.75s/it]

Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:05<00:05,  2.88s/it]
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:28 model_runner.py:1115] Loading model weights took 3.4835 GB
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:29 model_runner.py:1115] Loading model weights took 3.4835 GB

Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:08<00:02,  2.67s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  1.89s/it]

Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:08<00:00,  2.21s/it]

[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:30 model_runner.py:1115] Loading model weights took 3.4835 GB
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:31 model_runner.py:1115] Loading model weights took 3.4835 GB
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:31 model_runner.py:1115] Loading model weights took 3.4835 GB
[2025-04-15 01:47:31,081] [[32m    INFO[0m]: Loading model weights took 3.4835 GB (model_runner.py:1115)[0m
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:31 model_runner.py:1115] Loading model weights took 3.4835 GB
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:31 model_runner.py:1115] Loading model weights took 3.4835 GB
[1;36m(VllmWorkerProcess pid=368668)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=368668)[0;0m DEBUG - CHOT parameters: steps=0, lr=0.1
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:40 worker.py:267] Memory profiling takes 8.87 seconds
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:40 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:40 worker.py:267] model weights take 3.48GiB; non_torch_memory takes 3.48GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 62.42GiB.
[1;36m(VllmWorkerProcess pid=368672)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=368672)[0;0m DEBUG - CHOT parameters: steps=0, lr=0.1
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:40 worker.py:267] Memory profiling takes 8.86 seconds
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:40 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:40 worker.py:267] model weights take 3.48GiB; non_torch_memory takes 3.17GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 62.73GiB.
[1;36m(VllmWorkerProcess pid=368666)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=368666)[0;0m DEBUG - CHOT parameters: steps=0, lr=0.1
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:40 worker.py:267] Memory profiling takes 8.88 seconds
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:40 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:40 worker.py:267] model weights take 3.48GiB; non_torch_memory takes 3.48GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 62.42GiB.
[1;36m(VllmWorkerProcess pid=368667)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=368667)[0;0m DEBUG - CHOT parameters: steps=0, lr=0.1
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:40 worker.py:267] Memory profiling takes 8.90 seconds
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:40 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:40 worker.py:267] model weights take 3.48GiB; non_torch_memory takes 3.48GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 62.42GiB.
[1;36m(VllmWorkerProcess pid=368670)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=368670)[0;0m DEBUG - CHOT parameters: steps=0, lr=0.1
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:40 worker.py:267] Memory profiling takes 8.81 seconds
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:40 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:40 worker.py:267] model weights take 3.48GiB; non_torch_memory takes 3.48GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 62.42GiB.
[1;36m(VllmWorkerProcess pid=368669)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=368669)[0;0m DEBUG - CHOT parameters: steps=0, lr=0.1
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:40 worker.py:267] Memory profiling takes 8.88 seconds
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:40 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:40 worker.py:267] model weights take 3.48GiB; non_torch_memory takes 3.48GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 62.42GiB.
[1;36m(VllmWorkerProcess pid=368671)[0;0m DEBUG - Starting CHOT optimization
[1;36m(VllmWorkerProcess pid=368671)[0;0m DEBUG - CHOT parameters: steps=0, lr=0.1
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:40 worker.py:267] Memory profiling takes 8.87 seconds
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:40 worker.py:267] the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:40 worker.py:267] model weights take 3.48GiB; non_torch_memory takes 3.48GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 62.42GiB.
[2025-04-15 01:47:40,910] [[32m    INFO[0m]: Memory profiling takes 9.48 seconds
the current vLLM instance can use total_gpu_memory (79.22GiB) x gpu_memory_utilization (0.90) = 71.29GiB
model weights take 3.48GiB; non_torch_memory takes 4.48GiB; PyTorch activation peak memory takes 1.91GiB; the rest of the memory reserved for KV Cache is 61.42GiB. (worker.py:267)[0m
[2025-04-15 01:47:41,107] [[32m    INFO[0m]: # CUDA blocks: 167720, # CPU blocks: 10922 (executor_base.py:110)[0m
[2025-04-15 01:47:41,107] [[32m    INFO[0m]: Maximum concurrency for 32768 tokens per request: 81.89x (executor_base.py:115)[0m
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[2025-04-15 01:47:44,266] [[32m    INFO[0m]: Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage. (model_runner.py:1434)[0m
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1

Capturing CUDA graph shapes:   0%|          | 0/19 [00:00<?, ?it/s][1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.

Capturing CUDA graph shapes:   5%|â–Œ         | 1/19 [00:00<00:16,  1.12it/s]
Capturing CUDA graph shapes:  11%|â–ˆ         | 2/19 [00:01<00:11,  1.52it/s]
Capturing CUDA graph shapes:  16%|â–ˆâ–Œ        | 3/19 [00:01<00:09,  1.70it/s]
Capturing CUDA graph shapes:  21%|â–ˆâ–ˆ        | 4/19 [00:02<00:08,  1.68it/s]
Capturing CUDA graph shapes:  26%|â–ˆâ–ˆâ–‹       | 5/19 [00:03<00:08,  1.74it/s]
Capturing CUDA graph shapes:  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:03<00:07,  1.81it/s]
Capturing CUDA graph shapes:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 7/19 [00:04<00:07,  1.67it/s]
Capturing CUDA graph shapes:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:04<00:06,  1.64it/s]
Capturing CUDA graph shapes:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 9/19 [00:05<00:05,  1.74it/s]
Capturing CUDA graph shapes:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:05<00:05,  1.77it/s]
Capturing CUDA graph shapes:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 11/19 [00:06<00:04,  1.73it/s]
Capturing CUDA graph shapes:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:07<00:03,  1.78it/s]
Capturing CUDA graph shapes:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 13/19 [00:07<00:03,  1.83it/s]
Capturing CUDA graph shapes:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:08<00:02,  1.75it/s]
Capturing CUDA graph shapes:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 15/19 [00:08<00:02,  1.79it/s]
Capturing CUDA graph shapes:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:09<00:01,  1.85it/s][1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:53 custom_all_reduce.py:226] Registering 1843 cuda graph addresses

Capturing CUDA graph shapes:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 17/19 [00:09<00:01,  1.70it/s][1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:54 custom_all_reduce.py:226] Registering 1843 cuda graph addresses
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:54 custom_all_reduce.py:226] Registering 1843 cuda graph addresses
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:54 custom_all_reduce.py:226] Registering 1843 cuda graph addresses

Capturing CUDA graph shapes:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:10<00:00,  1.78it/s][1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:54 custom_all_reduce.py:226] Registering 1843 cuda graph addresses
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:54 custom_all_reduce.py:226] Registering 1843 cuda graph addresses
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:55 custom_all_reduce.py:226] Registering 1843 cuda graph addresses

Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.71it/s]
Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:11<00:00,  1.72it/s]
[2025-04-15 01:47:55,327] [[32m    INFO[0m]: Registering 1843 cuda graph addresses (custom_all_reduce.py:226)[0m
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:47:55 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.20 GiB
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:47:56 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.20 GiB
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:47:56 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.20 GiB
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:47:56 model_runner.py:1562] Graph capturing finished in 11 secs, took 0.20 GiB
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:47:56 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.20 GiB
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:47:56 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.20 GiB
[2025-04-15 01:47:56,068] [[32m    INFO[0m]: Graph capturing finished in 12 secs, took 0.20 GiB (model_runner.py:1562)[0m
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:47:56 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.20 GiB
[2025-04-15 01:47:56,072] [[32m    INFO[0m]: init engine (profile, create kv cache, warmup model) took 24.86 seconds (llm_engine.py:431)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: --- INIT SEEDS --- (pipeline.py:263)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: --- LOADING TASKS --- (pipeline.py:216)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/ifeval/main.py (registry.py:142)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: Found 6 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/tiny_benchmarks/main.py (registry.py:142)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mt_bench/main.py (registry.py:142)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: Found 4 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/mix_eval/main.py (registry.py:142)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: Found 5 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/olympiade_bench/main.py (registry.py:142)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: Found 1 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/hle/main.py (registry.py:142)[0m
[2025-04-15 01:47:57,499] [[32m    INFO[0m]: Found 21 custom tasks in ~/Projects/nips25_slot/open-r2/openr1/lib/python3.11/site-packages/lighteval/tasks/extended/lcb/main.py (registry.py:142)[0m
[2025-04-15 01:47:57,501] [[32m    INFO[0m]: Idavidrein/gpqa gpqa_diamond (lighteval_task.py:187)[0m
[2025-04-15 01:47:57,501] [[33m WARNING[0m]: Careful, the task lighteval|gpqa:diamond is using evaluation data to build the few shot examples. (lighteval_task.py:260)[0m
Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub
[2025-04-15 01:48:21,344] [[33m WARNING[0m]: Using the latest cached version of the dataset since Idavidrein/gpqa couldn't be found on the Hugging Face Hub (load.py:1631)[0m
Found the latest cached dataset configuration 'gpqa_diamond' at ~/.cache/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Sat Apr 12 16:41:50 2025).
[2025-04-15 01:48:21,346] [[33m WARNING[0m]: Found the latest cached dataset configuration 'gpqa_diamond' at ~/.cache/huggingface/datasets/Idavidrein___gpqa/gpqa_diamond/0.0.0/90b8e5be2b1d3d2dbfe016cdab47981150600c4a (last modified on Sat Apr 12 16:41:50 2025). (cache.py:95)[0m
[2025-04-15 01:48:21,497] [[32m    INFO[0m]: --- RUNNING MODEL --- (pipeline.py:468)[0m
[2025-04-15 01:48:21,497] [[32m    INFO[0m]: Running RequestType.GREEDY_UNTIL requests (pipeline.py:472)[0m
[2025-04-15 01:48:21,550] [[33m WARNING[0m]: You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring. (data.py:260)[0m
Idavidrein/gpqa
gpqa_diamond
True
None

Splits:   0%|          | 0/1 [00:00<?, ?it/s][2025-04-15 01:48:21,600] [[33m WARNING[0m]: context_size + max_new_tokens=35585 which is greater than self.max_length=32768. Truncating context to 0 tokens. (vllm_model.py:270)[0m


Processed prompts:   0%|          | 0/198 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A

Processed prompts:   1%|          | 1/198 [00:21<1:11:39, 21.82s/it, est. speed input: 13.01 toks/s, output: 52.65 toks/s][A

Processed prompts:   1%|          | 2/198 [00:24<33:37, 10.29s/it, est. speed input: 20.63 toks/s, output: 100.44 toks/s] [A

Processed prompts:   2%|â–         | 3/198 [00:27<23:08,  7.12s/it, est. speed input: 27.20 toks/s, output: 140.81 toks/s][A

Processed prompts:   2%|â–         | 4/198 [00:28<15:29,  4.79s/it, est. speed input: 33.17 toks/s, output: 187.41 toks/s][A

Processed prompts:   3%|â–Ž         | 5/198 [00:31<13:04,  4.07s/it, est. speed input: 40.30 toks/s, output: 223.35 toks/s][A

Processed prompts:   3%|â–Ž         | 6/198 [00:33<10:24,  3.25s/it, est. speed input: 46.69 toks/s, output: 264.49 toks/s][A

Processed prompts:   4%|â–Ž         | 7/198 [00:34<08:16,  2.60s/it, est. speed input: 53.00 toks/s, output: 307.22 toks/s][A

Processed prompts:   4%|â–         | 8/198 [00:37<09:14,  2.92s/it, est. speed input: 54.45 toks/s, output: 330.34 toks/s][A

Processed prompts:   5%|â–         | 9/198 [00:40<08:59,  2.86s/it, est. speed input: 56.22 toks/s, output: 360.41 toks/s][A

Processed prompts:   5%|â–Œ         | 10/198 [00:40<06:27,  2.06s/it, est. speed input: 65.69 toks/s, output: 410.12 toks/s][A

Processed prompts:   6%|â–Œ         | 11/198 [00:42<05:39,  1.82s/it, est. speed input: 73.99 toks/s, output: 449.91 toks/s][A

Processed prompts:   6%|â–Œ         | 12/198 [00:46<08:16,  2.67s/it, est. speed input: 73.14 toks/s, output: 456.57 toks/s][A

Processed prompts:   7%|â–‹         | 13/198 [00:47<06:50,  2.22s/it, est. speed input: 75.57 toks/s, output: 496.37 toks/s][A

Processed prompts:   7%|â–‹         | 14/198 [00:49<05:49,  1.90s/it, est. speed input: 78.62 toks/s, output: 535.56 toks/s][A

Processed prompts:   8%|â–Š         | 15/198 [00:50<04:59,  1.64s/it, est. speed input: 81.81 toks/s, output: 575.57 toks/s][A

Processed prompts:   9%|â–Š         | 17/198 [00:52<04:17,  1.42s/it, est. speed input: 89.12 toks/s, output: 649.48 toks/s][A

Processed prompts:   9%|â–‰         | 18/198 [00:52<03:28,  1.16s/it, est. speed input: 93.19 toks/s, output: 695.81 toks/s][A

Processed prompts:  10%|â–ˆ         | 20/198 [00:55<03:26,  1.16s/it, est. speed input: 98.87 toks/s, output: 742.93 toks/s][A

Processed prompts:  11%|â–ˆ         | 21/198 [00:58<04:41,  1.59s/it, est. speed input: 99.94 toks/s, output: 754.81 toks/s][A

Processed prompts:  11%|â–ˆ         | 22/198 [01:00<05:20,  1.82s/it, est. speed input: 99.03 toks/s, output: 746.72 toks/s][A

Processed prompts:  12%|â–ˆâ–        | 23/198 [01:04<06:30,  2.23s/it, est. speed input: 98.55 toks/s, output: 757.47 toks/s][A

Processed prompts:  12%|â–ˆâ–        | 24/198 [01:08<07:59,  2.75s/it, est. speed input: 95.59 toks/s, output: 761.61 toks/s][A

Processed prompts:  13%|â–ˆâ–Ž        | 25/198 [01:12<08:47,  3.05s/it, est. speed input: 93.92 toks/s, output: 771.28 toks/s][A

Processed prompts:  13%|â–ˆâ–Ž        | 26/198 [01:15<08:57,  3.12s/it, est. speed input: 92.77 toks/s, output: 787.32 toks/s][A

Processed prompts:  14%|â–ˆâ–Ž        | 27/198 [01:15<06:30,  2.28s/it, est. speed input: 95.44 toks/s, output: 834.90 toks/s][A

Processed prompts:  14%|â–ˆâ–        | 28/198 [01:24<11:57,  4.22s/it, est. speed input: 89.97 toks/s, output: 795.81 toks/s][A

Processed prompts:  15%|â–ˆâ–        | 29/198 [01:26<10:09,  3.61s/it, est. speed input: 90.37 toks/s, output: 825.03 toks/s][A

Processed prompts:  15%|â–ˆâ–Œ        | 30/198 [01:27<07:43,  2.76s/it, est. speed input: 92.82 toks/s, output: 866.90 toks/s][A

Processed prompts:  16%|â–ˆâ–Œ        | 31/198 [01:31<09:03,  3.25s/it, est. speed input: 90.42 toks/s, output: 847.76 toks/s][A

Processed prompts:  16%|â–ˆâ–Œ        | 32/198 [01:32<06:29,  2.35s/it, est. speed input: 92.96 toks/s, output: 894.27 toks/s][A

Processed prompts:  17%|â–ˆâ–‹        | 33/198 [01:32<04:52,  1.77s/it, est. speed input: 96.18 toks/s, output: 938.72 toks/s][A

Processed prompts:  17%|â–ˆâ–‹        | 34/198 [01:34<05:02,  1.85s/it, est. speed input: 96.12 toks/s, output: 940.12 toks/s][A

Processed prompts:  18%|â–ˆâ–Š        | 35/198 [01:38<07:02,  2.59s/it, est. speed input: 94.45 toks/s, output: 947.14 toks/s][A

Processed prompts:  18%|â–ˆâ–Š        | 36/198 [01:40<05:48,  2.15s/it, est. speed input: 96.21 toks/s, output: 984.82 toks/s][A

Processed prompts:  19%|â–ˆâ–Š        | 37/198 [01:43<07:04,  2.64s/it, est. speed input: 94.89 toks/s, output: 996.98 toks/s][A

Processed prompts:  19%|â–ˆâ–‰        | 38/198 [01:45<05:56,  2.23s/it, est. speed input: 95.76 toks/s, output: 1032.84 toks/s][A

Processed prompts:  20%|â–ˆâ–‰        | 39/198 [01:45<04:51,  1.84s/it, est. speed input: 97.05 toks/s, output: 1071.89 toks/s][A

Processed prompts:  20%|â–ˆâ–ˆ        | 40/198 [01:46<03:49,  1.46s/it, est. speed input: 98.86 toks/s, output: 1114.08 toks/s][A

Processed prompts:  21%|â–ˆâ–ˆ        | 41/198 [01:47<03:19,  1.27s/it, est. speed input: 99.95 toks/s, output: 1139.24 toks/s][A

Processed prompts:  21%|â–ˆâ–ˆ        | 42/198 [01:48<03:23,  1.31s/it, est. speed input: 100.41 toks/s, output: 1149.42 toks/s][A

Processed prompts:  22%|â–ˆâ–ˆâ–       | 43/198 [01:49<03:02,  1.18s/it, est. speed input: 101.45 toks/s, output: 1188.04 toks/s][A

Processed prompts:  22%|â–ˆâ–ˆâ–       | 44/198 [01:52<04:28,  1.74s/it, est. speed input: 101.47 toks/s, output: 1203.34 toks/s][A

Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 45/198 [01:53<03:21,  1.32s/it, est. speed input: 103.52 toks/s, output: 1247.54 toks/s][A

Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 46/198 [01:54<03:39,  1.44s/it, est. speed input: 104.04 toks/s, output: 1276.09 toks/s][A

Processed prompts:  24%|â–ˆâ–ˆâ–Ž       | 47/198 [01:54<02:37,  1.04s/it, est. speed input: 106.64 toks/s, output: 1322.29 toks/s][A

Processed prompts:  24%|â–ˆâ–ˆâ–       | 48/198 [01:56<03:11,  1.28s/it, est. speed input: 106.94 toks/s, output: 1348.75 toks/s][A

Processed prompts:  25%|â–ˆâ–ˆâ–       | 49/198 [01:57<02:57,  1.19s/it, est. speed input: 107.49 toks/s, output: 1348.44 toks/s][A

Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 50/198 [01:58<02:49,  1.15s/it, est. speed input: 108.90 toks/s, output: 1383.50 toks/s][A

Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 51/198 [02:00<03:31,  1.44s/it, est. speed input: 109.00 toks/s, output: 1405.87 toks/s][A

Processed prompts:  26%|â–ˆâ–ˆâ–‹       | 52/198 [02:03<04:38,  1.91s/it, est. speed input: 108.41 toks/s, output: 1418.14 toks/s][A

Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 53/198 [02:06<05:11,  2.15s/it, est. speed input: 112.47 toks/s, output: 1433.86 toks/s][A

Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 54/198 [02:07<04:34,  1.90s/it, est. speed input: 112.72 toks/s, output: 1443.94 toks/s][A

Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 55/198 [02:11<05:36,  2.35s/it, est. speed input: 111.61 toks/s, output: 1452.23 toks/s][A

Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 56/198 [02:12<04:59,  2.11s/it, est. speed input: 111.99 toks/s, output: 1480.85 toks/s][A

Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 57/198 [02:14<04:43,  2.01s/it, est. speed input: 112.90 toks/s, output: 1506.54 toks/s][A

Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 58/198 [02:14<03:26,  1.48s/it, est. speed input: 114.52 toks/s, output: 1549.29 toks/s][A

Processed prompts:  30%|â–ˆâ–ˆâ–‰       | 59/198 [02:15<02:31,  1.09s/it, est. speed input: 115.79 toks/s, output: 1577.63 toks/s][A

Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 60/198 [02:18<04:25,  1.92s/it, est. speed input: 114.71 toks/s, output: 1578.80 toks/s][A

Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 61/198 [02:20<03:57,  1.73s/it, est. speed input: 114.90 toks/s, output: 1584.58 toks/s][A

Processed prompts:  31%|â–ˆâ–ˆâ–ˆâ–      | 62/198 [02:21<03:42,  1.64s/it, est. speed input: 115.11 toks/s, output: 1598.05 toks/s][A

Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/198 [02:21<02:05,  1.07it/s, est. speed input: 117.74 toks/s, output: 1661.74 toks/s][A

Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/198 [02:28<05:09,  2.33s/it, est. speed input: 114.01 toks/s, output: 1632.36 toks/s][A

Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/198 [02:29<04:19,  1.96s/it, est. speed input: 114.42 toks/s, output: 1636.11 toks/s][A

Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 67/198 [02:30<03:56,  1.81s/it, est. speed input: 114.43 toks/s, output: 1632.71 toks/s][A

Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/198 [02:30<02:58,  1.37s/it, est. speed input: 115.28 toks/s, output: 1639.12 toks/s][A

Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–      | 69/198 [02:31<02:38,  1.23s/it, est. speed input: 116.16 toks/s, output: 1673.35 toks/s][A

Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/198 [02:35<04:23,  2.06s/it, est. speed input: 115.94 toks/s, output: 1672.69 toks/s][A

Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/198 [02:41<06:43,  3.17s/it, est. speed input: 113.28 toks/s, output: 1654.97 toks/s][A

Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 72/198 [02:43<05:59,  2.85s/it, est. speed input: 113.61 toks/s, output: 1676.67 toks/s][A

Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 73/198 [02:49<07:44,  3.72s/it, est. speed input: 111.28 toks/s, output: 1662.10 toks/s][A

Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/198 [02:49<05:32,  2.68s/it, est. speed input: 112.95 toks/s, output: 1702.39 toks/s][A

Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/198 [02:51<05:02,  2.46s/it, est. speed input: 112.70 toks/s, output: 1707.51 toks/s][A

Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/198 [02:57<06:38,  3.26s/it, est. speed input: 110.96 toks/s, output: 1699.88 toks/s][A

Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 77/198 [02:57<04:44,  2.35s/it, est. speed input: 112.79 toks/s, output: 1739.81 toks/s][A

Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/198 [03:04<07:49,  3.92s/it, est. speed input: 109.41 toks/s, output: 1710.11 toks/s][A

Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/198 [03:05<05:54,  2.98s/it, est. speed input: 109.90 toks/s, output: 1724.27 toks/s][A

Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/198 [03:07<05:01,  2.55s/it, est. speed input: 109.68 toks/s, output: 1718.04 toks/s][A

Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/198 [03:08<04:09,  2.13s/it, est. speed input: 109.98 toks/s, output: 1734.77 toks/s][A

Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 82/198 [03:09<03:35,  1.85s/it, est. speed input: 114.09 toks/s, output: 1765.10 toks/s][A

Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/198 [03:13<05:00,  2.61s/it, est. speed input: 112.30 toks/s, output: 1738.42 toks/s][A

Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/198 [03:14<04:01,  2.12s/it, est. speed input: 112.55 toks/s, output: 1744.55 toks/s][A

Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/198 [03:15<02:28,  1.33s/it, est. speed input: 114.70 toks/s, output: 1819.37 toks/s][A

Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 87/198 [03:16<02:17,  1.23s/it, est. speed input: 115.80 toks/s, output: 1851.58 toks/s][A

Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/198 [03:16<01:49,  1.01it/s, est. speed input: 116.39 toks/s, output: 1861.60 toks/s][A

Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/198 [03:18<01:51,  1.02s/it, est. speed input: 116.45 toks/s, output: 1862.57 toks/s][A

Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/198 [03:19<01:52,  1.04s/it, est. speed input: 116.63 toks/s, output: 1870.15 toks/s][A

Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/198 [03:19<01:45,  1.02it/s, est. speed input: 116.77 toks/s, output: 1871.70 toks/s][A

Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 92/198 [03:21<02:03,  1.17s/it, est. speed input: 116.76 toks/s, output: 1884.80 toks/s][A

Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/198 [03:23<02:18,  1.32s/it, est. speed input: 116.92 toks/s, output: 1910.00 toks/s][A

Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/198 [03:23<01:54,  1.10s/it, est. speed input: 118.09 toks/s, output: 1945.34 toks/s][A

Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/198 [03:25<02:00,  1.17s/it, est. speed input: 118.00 toks/s, output: 1943.72 toks/s][A

Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/198 [03:26<02:00,  1.18s/it, est. speed input: 117.94 toks/s, output: 1943.34 toks/s][A

Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 97/198 [03:28<02:33,  1.52s/it, est. speed input: 117.38 toks/s, output: 1937.37 toks/s][A

Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/198 [03:32<03:44,  2.24s/it, est. speed input: 115.80 toks/s, output: 1912.31 toks/s][A

Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/198 [03:34<02:30,  1.54s/it, est. speed input: 116.55 toks/s, output: 1937.06 toks/s][A

Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/198 [03:36<02:55,  1.81s/it, est. speed input: 116.30 toks/s, output: 1953.89 toks/s][A

Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 102/198 [03:37<02:41,  1.68s/it, est. speed input: 116.00 toks/s, output: 1951.25 toks/s][A

Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/198 [03:39<02:43,  1.72s/it, est. speed input: 116.60 toks/s, output: 1975.15 toks/s][A

Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 104/198 [03:40<02:11,  1.39s/it, est. speed input: 117.35 toks/s, output: 2010.41 toks/s][A

Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/198 [03:43<02:55,  1.89s/it, est. speed input: 116.40 toks/s, output: 1998.28 toks/s][A

Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/198 [03:57<08:05,  5.27s/it, est. speed input: 110.67 toks/s, output: 1923.51 toks/s][A

Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 107/198 [04:02<08:14,  5.44s/it, est. speed input: 109.41 toks/s, output: 1916.65 toks/s][A

Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/198 [04:04<06:29,  4.32s/it, est. speed input: 110.14 toks/s, output: 1942.89 toks/s][A

Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 109/198 [04:07<05:36,  3.79s/it, est. speed input: 111.23 toks/s, output: 1962.35 toks/s][A

Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/198 [04:07<04:07,  2.82s/it, est. speed input: 111.81 toks/s, output: 1991.55 toks/s][A

Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/198 [04:10<04:18,  2.97s/it, est. speed input: 111.41 toks/s, output: 2004.17 toks/s][A

Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 112/198 [04:11<03:11,  2.23s/it, est. speed input: 113.27 toks/s, output: 2039.40 toks/s][A

Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/198 [04:12<02:34,  1.82s/it, est. speed input: 114.33 toks/s, output: 2071.42 toks/s][A

Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 114/198 [04:14<02:49,  2.01s/it, est. speed input: 114.04 toks/s, output: 2090.41 toks/s][A

Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/198 [04:18<03:35,  2.60s/it, est. speed input: 112.90 toks/s, output: 2077.61 toks/s][A

Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/198 [04:19<02:39,  1.94s/it, est. speed input: 113.65 toks/s, output: 2113.19 toks/s][A

Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 117/198 [04:25<04:19,  3.21s/it, est. speed input: 111.64 toks/s, output: 2086.01 toks/s][A

Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/198 [04:26<03:21,  2.52s/it, est. speed input: 113.20 toks/s, output: 2117.60 toks/s][A

Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 119/198 [04:27<02:59,  2.28s/it, est. speed input: 114.00 toks/s, output: 2142.88 toks/s][A

Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/198 [04:28<02:15,  1.73s/it, est. speed input: 115.03 toks/s, output: 2177.88 toks/s][A

Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/198 [04:29<01:57,  1.52s/it, est. speed input: 115.11 toks/s, output: 2186.01 toks/s][A

Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/198 [04:34<03:17,  2.59s/it, est. speed input: 113.90 toks/s, output: 2184.14 toks/s][A

Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/198 [04:37<03:18,  2.65s/it, est. speed input: 113.83 toks/s, output: 2200.80 toks/s][A

Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 124/198 [04:39<03:00,  2.44s/it, est. speed input: 113.97 toks/s, output: 2224.11 toks/s][A

Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/198 [04:40<02:27,  2.02s/it, est. speed input: 114.18 toks/s, output: 2243.95 toks/s][A

Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/198 [04:42<02:19,  1.94s/it, est. speed input: 114.91 toks/s, output: 2268.39 toks/s][A

Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 127/198 [04:43<02:06,  1.78s/it, est. speed input: 114.66 toks/s, output: 2272.39 toks/s][A

Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/198 [04:45<02:19,  1.99s/it, est. speed input: 114.70 toks/s, output: 2291.16 toks/s][A

Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/198 [04:48<01:48,  1.59s/it, est. speed input: 116.93 toks/s, output: 2349.86 toks/s][A

Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/198 [04:48<01:26,  1.29s/it, est. speed input: 117.72 toks/s, output: 2385.44 toks/s][A

Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 132/198 [04:50<01:35,  1.45s/it, est. speed input: 117.41 toks/s, output: 2387.28 toks/s][A

Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/198 [04:52<01:45,  1.62s/it, est. speed input: 117.53 toks/s, output: 2408.63 toks/s][A

Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/198 [04:58<02:22,  2.26s/it, est. speed input: 116.13 toks/s, output: 2399.82 toks/s][A

Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/198 [04:58<01:50,  1.79s/it, est. speed input: 117.95 toks/s, output: 2436.07 toks/s][A

Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 137/198 [05:00<01:42,  1.69s/it, est. speed input: 118.98 toks/s, output: 2463.26 toks/s][A

Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/198 [05:03<02:09,  2.16s/it, est. speed input: 118.27 toks/s, output: 2467.92 toks/s][A

Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 139/198 [05:04<01:40,  1.71s/it, est. speed input: 118.56 toks/s, output: 2483.96 toks/s][A

Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/198 [05:05<01:24,  1.46s/it, est. speed input: 119.13 toks/s, output: 2515.78 toks/s][A

Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/198 [05:05<01:01,  1.08s/it, est. speed input: 119.66 toks/s, output: 2544.10 toks/s][A

Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/198 [05:06<00:53,  1.03it/s, est. speed input: 121.29 toks/s, output: 2606.96 toks/s][A

Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 144/198 [05:07<00:41,  1.29it/s, est. speed input: 121.89 toks/s, output: 2643.88 toks/s][A

Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/198 [05:07<00:41,  1.26it/s, est. speed input: 123.10 toks/s, output: 2675.13 toks/s][A

Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/198 [05:12<01:34,  1.82s/it, est. speed input: 122.23 toks/s, output: 2674.02 toks/s][A

Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 147/198 [05:12<01:09,  1.37s/it, est. speed input: 122.88 toks/s, output: 2711.01 toks/s][A

Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/198 [05:15<01:22,  1.65s/it, est. speed input: 122.83 toks/s, output: 2729.09 toks/s][A

Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/198 [05:15<00:46,  1.04it/s, est. speed input: 123.97 toks/s, output: 2784.99 toks/s][A

Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 151/198 [05:16<00:50,  1.08s/it, est. speed input: 124.46 toks/s, output: 2810.76 toks/s][A

Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 152/198 [05:18<00:54,  1.19s/it, est. speed input: 124.39 toks/s, output: 2822.51 toks/s][A

Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/198 [05:20<01:10,  1.56s/it, est. speed input: 124.02 toks/s, output: 2834.81 toks/s][A

Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 154/198 [05:22<01:11,  1.62s/it, est. speed input: 123.83 toks/s, output: 2842.16 toks/s][A

Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/198 [05:23<00:56,  1.32s/it, est. speed input: 124.35 toks/s, output: 2875.93 toks/s][A

Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 156/198 [05:24<00:51,  1.23s/it, est. speed input: 124.59 toks/s, output: 2905.41 toks/s][A

Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 157/198 [05:24<00:37,  1.10it/s, est. speed input: 125.25 toks/s, output: 2943.01 toks/s][A

Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/198 [05:25<00:41,  1.03s/it, est. speed input: 125.06 toks/s, output: 2949.63 toks/s][A

Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 159/198 [05:26<00:37,  1.04it/s, est. speed input: 125.55 toks/s, output: 2980.97 toks/s][A

Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/198 [05:33<01:41,  2.67s/it, est. speed input: 124.16 toks/s, output: 2959.90 toks/s][A

Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 161/198 [05:34<01:19,  2.15s/it, est. speed input: 124.85 toks/s, output: 2990.56 toks/s][A

Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/198 [05:39<01:20,  2.29s/it, est. speed input: 124.56 toks/s, output: 3005.51 toks/s][A

Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 164/198 [05:40<01:10,  2.06s/it, est. speed input: 124.67 toks/s, output: 3032.46 toks/s][A

Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/198 [05:41<01:01,  1.86s/it, est. speed input: 124.50 toks/s, output: 3040.73 toks/s][A

Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 166/198 [05:42<00:54,  1.70s/it, est. speed input: 124.80 toks/s, output: 3068.36 toks/s][A

Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 167/198 [05:44<00:49,  1.60s/it, est. speed input: 125.44 toks/s, output: 3095.39 toks/s][A

Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/198 [05:45<00:42,  1.42s/it, est. speed input: 125.80 toks/s, output: 3125.83 toks/s][A

Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 169/198 [05:46<00:41,  1.44s/it, est. speed input: 126.50 toks/s, output: 3151.52 toks/s][A

Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/198 [05:50<00:56,  2.02s/it, est. speed input: 126.08 toks/s, output: 3160.06 toks/s][A

Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 171/198 [05:51<00:45,  1.68s/it, est. speed input: 126.33 toks/s, output: 3191.23 toks/s][A

Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 172/198 [05:52<00:39,  1.50s/it, est. speed input: 126.74 toks/s, output: 3220.66 toks/s][A

Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/198 [05:53<00:39,  1.58s/it, est. speed input: 126.59 toks/s, output: 3232.06 toks/s][A

Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 174/198 [05:54<00:28,  1.20s/it, est. speed input: 127.13 toks/s, output: 3268.56 toks/s][A

Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/198 [05:56<00:36,  1.57s/it, est. speed input: 126.80 toks/s, output: 3278.74 toks/s][A

Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 176/198 [05:57<00:28,  1.30s/it, est. speed input: 127.04 toks/s, output: 3301.67 toks/s][A

Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 177/198 [05:57<00:20,  1.03it/s, est. speed input: 134.85 toks/s, output: 3339.19 toks/s][A

Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/198 [05:57<00:16,  1.21it/s, est. speed input: 135.11 toks/s, output: 3360.05 toks/s][A

Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 179/198 [05:59<00:22,  1.17s/it, est. speed input: 135.17 toks/s, output: 3380.93 toks/s][A

Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/198 [06:02<00:28,  1.56s/it, est. speed input: 134.65 toks/s, output: 3381.91 toks/s][A

Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 181/198 [06:09<00:54,  3.22s/it, est. speed input: 132.71 toks/s, output: 3357.22 toks/s][A

Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 182/198 [06:10<00:41,  2.62s/it, est. speed input: 132.71 toks/s, output: 3374.18 toks/s][A

Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/198 [06:13<00:39,  2.66s/it, est. speed input: 132.31 toks/s, output: 3389.83 toks/s][A

Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 184/198 [06:15<00:35,  2.52s/it, est. speed input: 131.82 toks/s, output: 3393.18 toks/s][A

Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/198 [06:16<00:26,  2.07s/it, est. speed input: 132.02 toks/s, output: 3424.55 toks/s][A

Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 186/198 [06:28<01:01,  5.08s/it, est. speed input: 128.56 toks/s, output: 3359.31 toks/s][A

Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 187/198 [06:34<00:58,  5.34s/it, est. speed input: 126.89 toks/s, output: 3333.91 toks/s][A

Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/198 [06:52<01:29,  8.96s/it, est. speed input: 122.00 toks/s, output: 3231.01 toks/s][A

Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 189/198 [06:53<01:01,  6.80s/it, est. speed input: 121.87 toks/s, output: 3248.36 toks/s][A

Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/198 [06:57<00:46,  5.82s/it, est. speed input: 121.18 toks/s, output: 3250.30 toks/s][A

Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 191/198 [07:05<00:44,  6.35s/it, est. speed input: 119.47 toks/s, output: 3231.31 toks/s][A

Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 192/198 [07:29<01:09, 11.65s/it, est. speed input: 113.49 toks/s, output: 3097.80 toks/s][A

Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/198 [07:33<00:47,  9.50s/it, est. speed input: 112.69 toks/s, output: 3100.84 toks/s][A

Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 194/198 [07:38<00:32,  8.06s/it, est. speed input: 111.80 toks/s, output: 3101.39 toks/s][A

Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/198 [07:55<00:32, 10.82s/it, est. speed input: 108.07 toks/s, output: 3024.79 toks/s][A

Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 196/198 [07:58<00:16,  8.37s/it, est. speed input: 107.88 toks/s, output: 3051.92 toks/s][A

Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 197/198 [09:55<00:41, 41.08s/it, est. speed input: 87.69 toks/s, output: 2504.15 toks/s] [A

Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [10:00<00:00, 30.25s/it, est. speed input: 87.31 toks/s, output: 2537.70 toks/s][A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [10:00<00:00,  3.03s/it, est. speed input: 87.31 toks/s, output: 2537.70 toks/s]

Splits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:01<00:00, 601.24s/it]
Splits: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:01<00:00, 601.24s/it]
[2025-04-15 01:58:23,599] [[32m    INFO[0m]: Terminating local vLLM worker processes (multiproc_worker_utils.py:141)[0m
[1;36m(VllmWorkerProcess pid=368666)[0;0m INFO 04-15 01:58:23 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=368667)[0;0m INFO 04-15 01:58:23 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=368669)[0;0m INFO 04-15 01:58:23 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=368671)[0;0m INFO 04-15 01:58:23 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=368668)[0;0m INFO 04-15 01:58:23 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=368672)[0;0m INFO 04-15 01:58:23 multiproc_worker_utils.py:253] Worker exiting
[1;36m(VllmWorkerProcess pid=368670)[0;0m INFO 04-15 01:58:23 multiproc_worker_utils.py:253] Worker exiting
[2025-04-15 01:58:25,819] [[32m    INFO[0m]: --- COMPUTING METRICS --- (pipeline.py:504)[0m
[2025-04-15 01:58:26,019] [[32m    INFO[0m]: --- DISPLAYING RESULTS --- (pipeline.py:546)[0m
[2025-04-15 01:58:26,029] [[32m    INFO[0m]: --- SAVING AND PUSHING RESULTS --- (pipeline.py:536)[0m
[2025-04-15 01:58:26,029] [[32m    INFO[0m]: Saving experiment tracker (evaluation_tracker.py:180)[0m
[2025-04-15 01:58:26,828] [[32m    INFO[0m]: Saving results to ~/Projects/nips25_slot/open-r2/data/evals~/Models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/results/_Models_deepseek-ai_DeepSeek-R1-Distill-Qwen-14B/results_2025-04-15T01-58-26.029326.json (evaluation_tracker.py:234)[0m
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
DEBUG - Starting CHOT optimization
DEBUG - CHOT parameters: steps=0, lr=0.1
|          Task          |Version|     Metric     |Value|   |Stderr|
|------------------------|------:|----------------|----:|---|-----:|
|all                     |       |extractive_match|0.601|Â±  |0.0349|
|lighteval:gpqa:diamond:0|      0|extractive_match|0.601|Â±  |0.0349|

